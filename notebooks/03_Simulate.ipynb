{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Simulation and combination\n",
        "\n",
        "In order to simulate re-assignment employee to a closer office, we calculate for each employee routing distance/duration between their address and all offices in the same province and confinant provinces. \n",
        "\n",
        "The output of the notebook is a dataframe with all distances calculated, in order to leave selection to business user or simply filter-out some offices. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, pandas_udf\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "harbour_path = 'path/to/curated_data'\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "  .master(\"local\")\\\n",
        "  .appName(\"application-name\")\\\n",
        "  .getOrCreate()\n",
        "\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# define udf\n",
        "# calculate distances from osm api\n",
        "def get_distances(origin_lat, origin_lon, destination_lat, destination_lon, by='car'):\n",
        "    try:\n",
        "        r = requests.get(\n",
        "            \"http://router.project-osrm.org/route/v1/{}/{},{};{},{}\"\\\n",
        "            .format(by, origin_lon, origin_lat, destination_lon, destination_lat)\n",
        "            )\n",
        "        response = json.loads(r.content)\n",
        "\n",
        "        distance=float(response['routes'][0].get('distance')) # in meters\n",
        "        duration=float(response['routes'][0].get('duration')) # in seconds\n",
        "    except:\n",
        "        distance=None\n",
        "        duration=None\n",
        "\n",
        "    return [distance, duration]\n",
        "\n",
        "# register as pandas udf\n",
        "@pandas_udf('distance long, duration long')\n",
        "def get_distances_udf(origin_lat: pd.Series, origin_lon: pd.Series, destination_lat: pd.Series, destination_lon: pd.Series) -> pd.DataFrame:\n",
        "    frame={\n",
        "        'origin_lat': origin_lat, # y\n",
        "        'origin_lon': origin_lon, # x\n",
        "        'destination_lat': destination_lat, # y\n",
        "        'destination_lon': destination_lon  # x\n",
        "        }\n",
        "  \n",
        "    result = pd.DataFrame(frame).apply(\n",
        "        lambda x: get_distances(x['origin_lat'], x['origin_lon'], x['destination_lat'], x['destination_lon']), \n",
        "        axis=1, result_type='expand'\n",
        "        )\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import clustered data\n",
        "anag_dip_geo_clus = spark.read.parquet(harbour_path + 'anag_dip_geo_clus')\n",
        "\n",
        "# import sedi data\n",
        "anag_sedi = spark.read.parquet(harbour_path + 'anag_sedi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# create combinations\n",
        "# per ogni provincia:\n",
        "# - creare la lista di province limitrofe in cui guardare;\n",
        "# - creare la lista di sedi appartenenti alla provincia e a quelle limitrofe;\n",
        "\n",
        "# lookup_prov_sedi: prov_act, prov_ass, id_sede_ass, geo_ass\n",
        "# prov\n",
        "# .withColumn('prov_act', array())\n",
        "# .join(anag_sedi, prov_ass == prov)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# combinazioni per ogni provincia delle province limitrofe\n",
        "comb_prov = spark.createDataFrame([\n",
        "    ('BO', ['BO', 'MO', 'FE', 'RA']),\n",
        "    ('FE', ['FE', 'MO', 'BO', 'RA']), \n",
        "    ('FC', ['FC', 'RN', 'RA']), \n",
        "    ('RA', ['RA', 'FC', 'BO','FE']), \n",
        "    ('RN', ['RN', 'FC']), \n",
        "    ('MO', ['MO', 'BO', 'FE', 'RE']), \n",
        "    ('RE', ['RE', 'MO', 'PR']),\n",
        "    ('PR', ['PR', 'RE', 'PC']), \n",
        "    ('PC', ['PC', 'PR'])\n",
        "    ], schema = ['prov_act', 'prov_ass'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sim_dip = anag_dip_geo_clus\\\n",
        "    .filter(F.col('flg_regione') == 'In regione')\\\n",
        "    .select('id_dipendente', F.col('lat').alias('dip_lat'), F.col('lon').alias('dip_lon'), F.col('provincia').alias('prov_act'))\n",
        "\n",
        "sim_dip.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sim_sedi = anag_sedi\\\n",
        "    .filter((F.col('flg_regione') == 'In regione') & (F.col('cancellato')==0))\\\n",
        "    .select('id_sede', F.col('lat').alias('sede_lat'),F.col('lon').alias('sede_lon'), F.col('provincia').alias('prov_ass'))\n",
        "\n",
        "sim_sedi.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# genero le combinazioni di coppie id_dipendente/id_sede della sua provincia o provincia confinante\n",
        "comb_dip_sedi = comb_prov\\\n",
        "    .select('prov_act', F.explode('prov_ass').alias('prov_ass'))\\\n",
        "    .join(sim_dip, on='prov_act')\\\n",
        "    .join(sim_sedi, on='prov_ass')\\\n",
        "    .select('id_dipendente', 'id_sede', 'dip_lat', 'dip_lon', 'sede_lat', 'sede_lon')\\\n",
        "    .distinct()\n",
        "    \n",
        "comb_dip_sedi.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Routing combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "comb_window = Window.partitionBy('id_dipendente').orderBy(F.col('distanza'))\n",
        "comb_dip_sedi_geo = comb_dip_sedi\\\n",
        "    .withColumn('Route', \n",
        "        get_distances_udf('dip_lat', 'dip_lon', 'sede_lat', 'sede_lon'))\\\n",
        "    .withColumn('distanza', col('Route.distance')/1000.0)\\\n",
        "    .withColumn('durata', col('Route.duration')/60.0)\\\n",
        "    .select('id_dipendente', 'id_sede', 'distanza', 'durata')\\\n",
        "    .withColumn('rank', F.row_number().over(comb_window))\n",
        "\n",
        "# count action to trigger computation (long running computation)\n",
        "# comb_dip_sedi_geo.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Persist data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# write back data (long computation)\n",
        "comb_dip_sedi_geo.write.parquet(harbour_path + 'comb_dip_sedi_geo', mode='overwrite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "description": null,
    "interpreter": {
      "hash": "a2176ed0753c912996b883d7e74abc00192f41831e7e7297deb7d9eae49ab795"
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 ('spark-py')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "save_output": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
